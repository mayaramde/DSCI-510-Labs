{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 10 (Week-11)\n",
    "-----------------------------------------\n",
    "Hey Everyone, \n",
    "\n",
    "Guidelines for the Lab:\n",
    "- Download the lab materials from D2L. You're supposed to complete it by the deadline, Sunday (27th March) 4pm PDT.  \n",
    "\n",
    "- You have to complete the assignments individually. If you are having trouble completing the assignment do let the TA's/CP's or Graders know.\n",
    "\n",
    "- Please don't copy code (either partially/fully) from your classmates. Plagiarism of any form will result in a 0 and you will be reported to SJACS (https://sjacs.usc.edu/students/academic-integrity/).\n",
    "\n",
    "- You have to fill the code in this notebook and upload it for submission. While doing this, make sure all supporting files that you download from D2L are in the same directory as this notebook.  \n",
    "\n",
    "- Please don't submit code in text files and .py files. Also no mailing code to the TA's or Graders. There were exemptions provided to students in earlier labs due to D2L issues, environment setup and course registrations but these exemptions will not be carried over to future labs.\n",
    "\n",
    "- File Naming Convention : Lab10_FIRSTNAME_LASTNAME.ipynb (Failure to comply to this policy may incur penalties ).\n",
    "\n",
    "- Don't change existing methods (if any, unless otherwise mentioned in the instructions)\n",
    "\n",
    "- You are encouraged to look up resources online like python docs and stackoverflow. But, you are encouraged to look up the topics and not the questions themselves.  \n",
    "\n",
    "- Your last submission before the deadline will be counted towards your grade. You can submit any number of times before the deadline.\n",
    "\n",
    "- Scores are capped at 10 points including the Bonus problem. For Example : Even if you get a score of 13/10, your score is capped at 10.\n",
    "\n",
    "- Your output should be as mentioned in the question (Don't output/print extra items which are not asked in the question as it makes it difficult for the graders to grade). (Failure to comply to this policy may incur penalties)\n",
    "\n",
    "- You need to take input from file/console only if it is mentioned in the question. If not we need just the logic/implementation of your function.\n",
    "\n",
    "- Please Don't ask for changes to the Grading Rubrics. Some questions have all/nothing rubric and missing one test case/ edge case may result in a 0 score. So you are encouraged to write your own test cases and evaluate your code as and where necessary. The TA's or Graders will not be providing you with the test cases.\n",
    "\n",
    "----------------------------------------------\n",
    "- To make coding easier, a lot of the boilerplate code has been given as a template to show you first hand how OOPs constructs are designed. Please use them as references.\n",
    "\n",
    "- Don't change the existing boilerplate code. The reason the boilerplate code has been given, is to make it easy for you to see the bigger picture and get you up to speed faster. There have been multiple comments added, to make it easier for students to logically comprehend the sequence of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with looking at JSON parsing using Python. The first couple questions are just to get you warmed up with the **json** library offered by Python to parse and process json data. Next, we will try to look at a real-life use case for parsing JSON data and trying to extract some statistics from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.[1 point] \n",
    "---\n",
    "\n",
    "Given a JSON string person, use the json library to parse the string into a dictionary and then *return* the languages spoken by the person.\n",
    "\n",
    "input to function : json string - person. <br>\n",
    "expected output   : list of languages spoken by person (in this case, Bob).\n",
    "\n",
    "---\n",
    "Grading Rubric: 2 points if the output matches expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_person_json(person):\n",
    "    person_dict = json.loads(person) # parse the JSON input\n",
    "    return person_dict['languages'] # values of 'language' key is the list of person's languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English', 'French']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_person_json('{\"name\": \"Bob\", \"languages\": [\"English\", \"French\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.[1 point] \n",
    "---\n",
    "\n",
    "Given a dictionary person, use the helper function to add some extra information about the person to the dictionary and then write the dictionary to a json file. After writing the dictionary to the json file, load the json file again and print the contents of the json file. The contents of the file should have the original input dictionary + the new features added using the helper function. \n",
    "\n",
    "input to function : dictionary - person. <br>\n",
    "expected output   : The json string that contains all the information about the person (all information = information from input dict  + new information from helper function)\n",
    "\n",
    "---\n",
    "Grading Rubric: 2 points if the output matches expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(person_dict, **additional_features):\n",
    "    for feature_name, value in additional_features.items():\n",
    "        person_dict[feature_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def print_json_string(person_dict, json_file_to_write, **additional_features):\n",
    "    # The helper function has already been called for you.\n",
    "    helper(person_dict, **additional_features)\n",
    "    # Start writing your code from this point.\n",
    "    \n",
    "    with open(json_file_to_write, 'w') as outfile: # write person_dict into json file\n",
    "        json.dump(person_dict, outfile) # dump person_dict into file\n",
    "        \n",
    "    with open(json_file_to_write, 'r') as infile: # open and read json file\n",
    "        data = json.load(infile) # load contents of file into new object\n",
    "    \n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Bob', 'languages': ['English', 'French'], 'age': 20, 'gender': 'Female', 'highest_degree': 'Graduate'}\n"
     ]
    }
   ],
   "source": [
    "person_dict = {'name': 'Bob', 'languages': ['English', 'French']}\n",
    "print_json_string(person_dict, 'json_data.json', age=20, gender='Female', highest_degree='Graduate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.[2 points]\n",
    "--- \n",
    "Let's look at how to perform some fundamental analysis on an address book JSON dataset. \n",
    "\n",
    "The dataset in question can be found here : https://www.kaggle.com/datasets/ahmedshahriarsakib/list-of-real-usa-addresses - You are highly encouraged to check out the data, the notebook thats' been created to analyze geospatial information on this data and to learn more about the Kaggle Data Science community as well!\n",
    "\n",
    "For our lab, we are only going to try and get a taste of how JSON data can be processed (pretty much the same as the first 2 questions) and how large data can be analyzed to gain some very basic insights on the data.\n",
    "\n",
    "For the purposes of this lab, we will again be looking at an object-oriented skeleton code and perform the following operations: \n",
    "\n",
    "- \\_\\_init__() - Load the JSON data from the json file into a dictionary which can be accessed across the class (using self).\n",
    "\n",
    "- find_maximum_state_entries() - iterate through the dictionary and return the state with the maximum number of entries in the address book.\n",
    "\n",
    "---\n",
    "Grading Rubric: 1 point each for implementing the init() and find_maximum_state_entries() methods correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "class Geo_Data_Analytics():\n",
    "    \n",
    "    def __init__(self, json_file):\n",
    "        # write your code here\n",
    "        with open(json_file, 'r') as f: # open and read json file\n",
    "            self.address_dict = json.load(f)\n",
    "\n",
    "    def find_maximum_state_entries(self):\n",
    "        state_entries = {}\n",
    "        \n",
    "        # iterate through address_dict and record frequency of states' entries\n",
    "        for address in self.address_dict:\n",
    "            state = address.get('state')\n",
    "            if state in state_entries:\n",
    "                state_entries[state] += 1\n",
    "            else:\n",
    "                state_entries[state] = 1\n",
    "                \n",
    "        # iterate through state_entries and find state with maximum # of entries\n",
    "        #max_entry = state_entries[0]\n",
    "        max_state = ''\n",
    "        max_entry = 0\n",
    "        \n",
    "        for state, count in state_entries.items():\n",
    "            if count > max_entry:\n",
    "                max_entry = count\n",
    "                max_state = state\n",
    "        # Inside the format, the first parameter will be the name of the state with maximum entries\n",
    "        # and the second parameter will be the count of the entries.\n",
    "        print('State {0} has the maximum number of entries in the address book : The count is {1}'.format(max_state, max_entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State AL has the maximum number of entries in the address book : The count is 101\n"
     ]
    }
   ],
   "source": [
    "s = Geo_Data_Analytics('list_of_real_usa_addresses.json')\n",
    "s.find_maximum_state_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's look at some XML data formats and see how we can process them to gain insights from XML data. \n",
    "\n",
    "Before attempting the below question, I recommend you to skim this xml documentation: https://docs.python.org/3/library/xml.etree.elementtree.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.\n",
    "---\n",
    "\n",
    "This task also deals with location data. In this case, we have a dataset of cities, the state they are in and the country they belong to.\n",
    "\n",
    "The dataset looks like this : There is a root tag called **data** which has many child tags in it called **city**. Each city tag *may have multiple attributes and multiple tags in it*. These tags and attributes are not mandated to be there in each city tag. Each city also has a class associated with it. The class can either be **Metropolitan** or **Town**.\n",
    "\n",
    "\n",
    "Assumptions: \n",
    "- All the attribute and tag names are combinedly unique.\n",
    "- All cities will have `name` tag in it. \n",
    "\n",
    "Note: While extracting the text from a tag, use `strip` to remove empty spaces and new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part (A) [Example]\n",
    "---\n",
    "\n",
    "For this part, let's just get a feel of the data and how the XML library parses and processes the data. Given the `city.xml` file, print all the cities that are part of the XML data.\n",
    "\n",
    "Input  : XML file. <br>\n",
    "Output : String of all cities separated by comma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def find_cities(xml_file):\n",
    "\n",
    "# Solution 1\n",
    "    city_list = []\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for city in root:\n",
    "        for tag in city:\n",
    "            if tag.tag == 'name':\n",
    "                city_list.append(tag.text.strip())\n",
    "                \n",
    "    return ','.join(city_list)\n",
    "\n",
    "# Solution 2 - using path expressions.\n",
    "# tree = ET.parse(xml_file)\n",
    "# ','.join(map(lambda x : x.text.strip(), tree.findall('.//name')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los Angeles,Paris,New York,Amherst,Noida'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_cities(\"city.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part (B)[4 points]\n",
    "---\n",
    "\n",
    "Here, we will explore the `lxml` library which provides a very easy and convenient way of querying the XML data to perform analytics and gain insights on the dataset. The `xpath` method allows a very clean way of querying XML data in python and can be used to navigate through elements and attributes in an XML document because of its 'path-like' syntax for querying data.\n",
    "\n",
    "The below data is a small summary of 3 countries, their rank, GDP per Capita (gdppc) and their neighbors. There are 4 queries that we need to perform on this dataset and for each query that is successfully executed, we will get 1 point.\n",
    "The XML data is listed below and it has already been parsed into an XML tree for you. The queries to be executed are mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "data = '''<?xml version=\"1.0\"?>\n",
    "<data>\n",
    "    <country name=\"Liechtenstein\">\n",
    "        <rank updated=\"yes\">2</rank>\n",
    "        <year>2008</year>\n",
    "        <gdppc>141100</gdppc>\n",
    "        <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "        <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "    </country>\n",
    "    <country name=\"Singapore\">\n",
    "        <rank updated=\"yes\">5</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>59900</gdppc>\n",
    "        <neighbor name=\"Colombia\" direction=\"N\"/>\n",
    "    </country>\n",
    "    <country name=\"Panama\">\n",
    "        <rank updated=\"yes\">69</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>13600</gdppc>\n",
    "        <neighbor name=\"Costa Rica\" direction=\"W\"/>\n",
    "        <neighbor name=\"Colombia\" direction=\"E\"/>\n",
    "    </country>\n",
    "</data>'''\n",
    "\n",
    "tree = etree.XML(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write an xpath query to find the GDP per capita of Singapore. The answer should be an **numerical value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59900']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath(\"country[@name='Singapore']/gdppc/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write an xpath query to find the list of countries that have GDP per capita > 100000. The answer should be the name of countries in a **list**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element country at 0x7f9d12698ac0>]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath(\"country[gdppc>100000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write an xpath query to find the list of countries with neighbor Colombia. The answer should be the name of countries in a **list**.\n",
    "\n",
    "Hint : You will be surprised at the answer returned! This goes to show that performing such queries and analytical operations on the data can also help uncover inconsistencies and errors in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element neighbor at 0x7f9d1269de40>, <Element neighbor at 0x7f9d12618ec0>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath(\"country/neighbor[@name = 'Colombia']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write an xpath query to find the list of countries that have rank <= 5. The answer should be the name of countries in a **list**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element country at 0x7f9d12698ac0>, <Element country at 0x7f9d1269ed80>]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath(\"country[rank <= 5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.[2 points] \n",
    "---\n",
    "To complete this question, you need the access key from the opencage website. To obtain that the procedure is:  \n",
    "1. open https://opencagedata.com/api\n",
    "2. Click on sign up for your free api key.\n",
    "3. Create a account and use the access key that you get from there for this question.\n",
    "\n",
    "\n",
    "Continuing from Q 4 (A), we have a comma separated list of all the cities in the XML file (you can directly call the method in Q 4(A) to get the comma separated list). We now have to get the latitude and longitude for the cities that you returned in the above question. Use the opencagedata api to do geocoding. Give the city name as the input and you will get a json response with lat and long information in it.   \n",
    "\n",
    "Input: XML file. <br>\n",
    "Return Output: List of tuples(each element in format (lat, long))  \n",
    "\n",
    "Note: Make use of the above question's function. If you cannot complete the above question, you can assume that it exists. \n",
    "\n",
    "---\n",
    "Grading Rubric: 1 point for generating the key and 1 point for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def find_cities_with_latlong(xml_file):\n",
    "    cities = find_cities(xml_file)\n",
    "    cities = cities.split(',')\n",
    "    api_key = \"e98726be11144a49b1c10f25fd7384f0\"\n",
    "    base_url = 'https://api.opencagedata.com/geocode/v1/json'\n",
    "    lst = []\n",
    "    \n",
    "    for city in cities:\n",
    "        enc_loc = requests.utils.quote(city)\n",
    "        url = f'{base_url}?q={enc_loc}&key={api_key}'\n",
    "        results = requests.get(url).json().get('results')\n",
    "        result = results[0]\n",
    "        x=result['annotations']['DMS']['lat'], result['annotations']['DMS']['lng']\n",
    "        lst.append(x)\n",
    "        \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"34° 3' 13.28724'' N\", \"118° 14' 33.95760'' W\"), (\"48° 51' 32.00292'' N\", \"2° 19' 12.14760'' E\"), (\"40° 42' 45.82116'' N\", \"74° 0' 21.65472'' W\"), (\"42° 22' 6.83688'' N\", \"72° 30' 20.57040'' W\"), (\"28° 34' 14.82276'' N\", \"77° 19' 37.58664'' E\")]\n"
     ]
    }
   ],
   "source": [
    "print(find_cities_with_latlong(\"city.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Question [2 points]\n",
    "---\n",
    "For this question, you are given a xml_file path and a `filters` dictionary as arguments of a function `filter_cities`. The key of the `filters` dictionary represents the attribute name of the city tag or the child tag name of the `city` tag. The value of that key represents the value of the corresponding attribute or text of the corresponding tag name.  \n",
    "\n",
    "This function should return the list of name of the cities that have matching values for all the attributes and tags given in the `filters`.  \n",
    "\n",
    "Input  : the filter dictionary and xml file. <br>\n",
    "Output : List of the cities that match the filter criteria. \n",
    "\n",
    "---\n",
    "Grading Rubric: 2 points if the output matches required output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "# Your code goes here\n",
    "def filter_cities(xml_file, filters):\n",
    "    cities = find_cities(xml_file)\n",
    "    cities = cities.split(',')\n",
    "    \n",
    "    country_match = []\n",
    "    \n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for key in filters.keys():\n",
    "        keys.append(key)\n",
    "        \n",
    "    attr_keys, child_keys = [],[]\n",
    "    \n",
    "    for city in root:\n",
    "        for tag in city:\n",
    "            if tag.tag == 'name':\n",
    "                city_list.append(tag.text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filters = {\n",
    "    \"class\": \"Metropolitan\",\n",
    "    \"country\": \"USA\"\n",
    "}\n",
    "print(filter_cities(\"city.xml\", filters))\n",
    "# Expected output: ['Los Angeles', 'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
